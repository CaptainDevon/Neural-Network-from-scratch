# Neural Network from Scratch using NumPy

## Overview

This repository contains an implementation of a neural network built from scratch using only NumPy and fundamental mathematics. No high-level frameworks like TensorFlow or PyTorch have been used, providing a deeper understanding of the inner workings of neural networks.

This repository has two notebooks
- **neural_network_scratch_rough_work**: this consists of the simple implementation for inderstanding the concepts
- **neural_network**: this is the actual implementation for the neural network usign math, Numpy and classes from OOPs concepts
## Features

- **Fully connected neural network**: Supports multiple layers and neurons.
- **Forward propagation**: Computes the output of the network given an input.
- **Backpropagation**: Adjusts the weights and biases of the network to minimize error.
- **Activation functions**: Includes commonly used activation functions such as Sigmoid, ReLU, and Tanh.
- **Loss functions**: Supports Mean Squared Error (MSE) and Cross-Entropy Loss.
- **Optimization**: Implements Gradient Descent for training the network.
- **Flexibility**: Easily adjustable architecture to experiment with different network configurations.
