{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]\n",
      "1.26.4\n",
      "3.9.0\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(np.__version__)\n",
    "print(plt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making of a sigle neuron (rough work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3000000000000003\n"
     ]
    }
   ],
   "source": [
    "inputs=[1,2,3]\n",
    "weights=[0.2,0.8,-0.5]\n",
    "bias=2\n",
    "result=bias+inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.800000000000001\n"
     ]
    }
   ],
   "source": [
    "inputs=[1,2,3,2.5]\n",
    "weights=[0.2,0.8,-0.5,1.0]\n",
    "bias=2\n",
    "result=bias+inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+inputs[3]*weights[3]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coding the layer (simple rough work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.800000000000001, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs=[1,2,3,2.5]\n",
    "weights1=[0.2,0.8,-0.5,1.0]\n",
    "weights2=[0.5,-0.91,0.26,-0.5]\n",
    "weights3=[-0.26,-0.27,0.17,0.87]\n",
    "\n",
    "bias1=2\n",
    "bias2=3\n",
    "bias3=0.5\n",
    "\n",
    "result=[bias1+inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3],\n",
    "        bias2+inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3],\n",
    "        bias3+inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+inputs[3]*weights3[3]]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning the rough work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs=[1,2,3,2.5]\n",
    "weights=[[0.2,0.8,-0.5,1.0],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "layer_output=[]\n",
    "for neuron_weights,neuron_bias in zip(weights,biases):\n",
    "    neuron_output=0\n",
    "    for n_input,weight in zip(inputs,neuron_weights):\n",
    "        neuron_output+=n_input*weight\n",
    "    neuron_output+=neuron_bias\n",
    "    layer_output.append(neuron_output)\n",
    "\n",
    "print(layer_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementing the dot product using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "inputs=[[1,2,3,2.5],\n",
    "        [2.0,5.0,-1.0,2.0],\n",
    "        [-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1.0],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "         \n",
    "biases=[2,3,0.5]\n",
    "\n",
    "layer1_outputs=np.dot(inputs,np.array(weights).T)+biases\n",
    "\n",
    "print(layer1_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031   0.53225 -2.03875]\n",
      " [ 0.2434  -2.6012  -5.7633 ]\n",
      " [-0.99314  1.4297  -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "weights2=[[0.1,-0.14,0.5],\n",
    "         [-0.5,0.12,0.33],\n",
    "         [-0.44,0.73,-0.13]]\n",
    "         \n",
    "biases2=[-1,2,-0.5]\n",
    "\n",
    "layer2_outputs=np.dot(layer1_outputs,np.array(weights2).T)+biases2\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning activation function from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "output= []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0,i))\n",
    "\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building softmax activation functions \n",
    "# these are the functions used on the output layer for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "layer_output=[4.8,1.21,2.385] \n",
    "E=math.e\n",
    "\n",
    "exp_values=[]\n",
    "for output in layer_output:\n",
    "    exp_values.append(E**output)\n",
    "\n",
    "print(exp_values)\n",
    "\n",
    "norm_base=sum(exp_values)\n",
    "norm_values=[]\n",
    "\n",
    "for norm in exp_values:\n",
    "    norm_values.append(norm/norm_base)\n",
    "\n",
    "print(norm_values)\n",
    "print(sum(norm_values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the hard code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.21510418e+02 3.35348465e+00 1.08590627e+01]\n",
      " [7.33197354e+03 1.63654137e-01 1.22140276e+00]\n",
      " [4.09595540e+00 2.86051020e+00 1.02634095e+00]]\n",
      "[[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n",
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "layer_output=[[4.8,1.21,2.385],\n",
    "              [8.9,-1.81,0.2],\n",
    "              [1.41,1.051,0.026]]\n",
    "exp_values=np.exp(layer_output)\n",
    "print(exp_values)\n",
    "\n",
    "print(np.sum(layer_output,axis=1,keepdims=True))\n",
    "norm_values=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "print(norm_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n",
      "0.35667494393873245\n",
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "softmax_output=[0.7,0.1,0.2]\n",
    "target_output=[1,0,0]\n",
    "\n",
    "\n",
    "loss=-(math.log(softmax_output[0])*target_output[0]+\n",
    "       math.log(softmax_output[1])*target_output[1]+\n",
    "       math.log(softmax_output[2])*target_output[2])\n",
    "\n",
    "print(loss)\n",
    "loss=-math.log(softmax_output[0])\n",
    "print(loss)\n",
    "print(-math.log(0.7))\n",
    "print(-math.log(0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "softmax_output=np.array([[0.7,0.1,0.2],\n",
    "                         [0.1,0.5,0.4],\n",
    "                         [0.02,0.9,0.88]])\n",
    "\n",
    "class_targets=[0,1,1]\n",
    "neg_loss=-np.log(softmax_output[range(len(softmax_output)),class_targets])\n",
    "\n",
    "average_loss=np.mean(neg_loss)\n",
    "print(average_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
